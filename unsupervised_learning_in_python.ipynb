{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "unsupervised-learning-in-python.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calypsobronte/DataCamp/blob/master/unsupervised_learning_in_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "2cer9csD7YS_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#1\n"
      ]
    },
    {
      "metadata": {
        "id": "ZQGHgJZL7jLJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import KMeans\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Create a KMeans instance with 3 clusters: model\n",
        "model = KMeans(n_clusters=3)\n",
        "\n",
        "# Fit model to points\n",
        "model.fit(points)\n",
        "\n",
        "# Determine the cluster labels of new_points: labels\n",
        "labels = model.predict(new_points)\n",
        "\n",
        "# Print cluster labels of new_points\n",
        "print(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-NlYdUZy69Z0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assign the columns of new_points: xs and ys\n",
        "xs = new_points[:, 0]\n",
        "ys = new_points[:, 1]\n",
        "\n",
        "# Make a scatter plot of xs and ys, using labels to define the colors\n",
        "plt.scatter(xs, ys, c=labels, alpha=0.5)\n",
        "\n",
        "# Assign the cluster centers: centroids\n",
        "centroids = model.cluster_centers_\n",
        "\n",
        "# Assign the columns of centroids: centroids_x, centroids_y\n",
        "centroids_x = centroids[:,0]\n",
        "centroids_y = centroids[:,1]\n",
        "\n",
        "# Make a scatter plot of centroids_x and centroids_y\n",
        "plt.scatter(centroids_x, centroids_y, marker='D', s=50)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6CfgegsN7Bky",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ks = range(1, 6)\n",
        "inertias = []\n",
        "\n",
        "for k in ks:\n",
        "    # Create a KMeans instance with k clusters: model\n",
        "    model = KMeans(n_clusters=k)\n",
        "    \n",
        "    # Fit model to samples\n",
        "    model.fit(samples)\n",
        "    \n",
        "    # Append the inertia to the list of inertias\n",
        "    inertias.append(model.inertia_)\n",
        "    \n",
        "# Plot ks vs inertias\n",
        "plt.plot(ks, inertias, '-o')\n",
        "plt.xlabel('number of clusters, k')\n",
        "plt.ylabel('inertia')\n",
        "plt.xticks(ks)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MHKlUCHW7ES6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a KMeans model with 3 clusters: model\n",
        "model = KMeans(n_clusters=3)\n",
        "\n",
        "# Use fit_predict to fit model and obtain cluster labels: labels\n",
        "labels = model.fit_predict(samples)\n",
        "\n",
        "# Create a DataFrame with labels and varieties as columns: df\n",
        "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
        "\n",
        "# Create crosstab: ct\n",
        "ct = pd.crosstab(df['labels'], df['varieties'])\n",
        "\n",
        "# Display ct\n",
        "print(ct)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "POLh76m57GGE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Perform the necessary imports\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Create scaler: scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Create KMeans instance: kmeans\n",
        "kmeans = KMeans(n_clusters=4)\n",
        "\n",
        "# Create pipeline: pipeline\n",
        "pipeline = make_pipeline(scaler, kmeans)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DKXB7E9q7JkM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Fit the pipeline to samples\n",
        "pipeline.fit(samples)\n",
        "\n",
        "# Calculate the cluster labels: labels\n",
        "labels = pipeline.predict(samples)\n",
        "\n",
        "# Create a DataFrame with labels and species as columns: df\n",
        "df = pd.DataFrame({'labels': labels, 'species': species})\n",
        "\n",
        "# Create crosstab: ct\n",
        "ct = pd.crosstab(df['labels'], df['species'])\n",
        "\n",
        "# Display ct\n",
        "print(ct)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "veEuCUI87LhV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import Normalizer\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "# Create a normalizer: normalizer\n",
        "normalizer = Normalizer()\n",
        "\n",
        "# Create a KMeans model with 10 clusters: kmeans\n",
        "kmeans = KMeans(n_clusters=10)\n",
        "\n",
        "# Make a pipeline chaining normalizer and kmeans: pipeline\n",
        "pipeline = make_pipeline(normalizer, kmeans)\n",
        "\n",
        "# Fit pipeline to the daily price movements\n",
        "pipeline.fit(movements)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GWpXszw57NgV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Predict the cluster labels: labels\n",
        "labels = pipeline.predict(movements)\n",
        "\n",
        "# Create a DataFrame aligning labels and companies: df\n",
        "df = pd.DataFrame({'labels': labels, 'companies': companies})\n",
        "\n",
        "# Display df sorted by cluster label\n",
        "print(df.sort_values('labels'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pJN7XAj97QlP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#2"
      ]
    },
    {
      "metadata": {
        "id": "z5yaY47i7S5X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Perform the necessary imports\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the linkage: mergings\n",
        "mergings = linkage(samples, method='complete')\n",
        "\n",
        "# Plot the dendrogram, using varieties as labels\n",
        "dendrogram(mergings,\n",
        "           labels=varieties,\n",
        "           leaf_rotation=90,\n",
        "           leaf_font_size=6,\n",
        ")\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gonMTScX7u3p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import normalize\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# Normalize the movements: normalized_movements\n",
        "normalized_movements = normalize(movements)\n",
        "\n",
        "# Calculate the linkage: mergings\n",
        "mergings = linkage(normalized_movements, 'complete')\n",
        "\n",
        "# Plot the dendrogram\n",
        "dendrogram(mergings, labels=companies, leaf_rotation=90, leaf_font_size=6)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cyeTCU9R7xEp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Perform the necessary imports\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "\n",
        "# Calculate the linkage: mergings\n",
        "mergings = linkage(samples, method='single')\n",
        "\n",
        "# Plot the dendrogram\n",
        "dendrogram(mergings, labels=country_names, leaf_rotation=90, leaf_font_size=6)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zLB6zOrc7y-k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Perform the necessary imports\n",
        "import pandas as pd\n",
        "from scipy.cluster.hierarchy import fcluster\n",
        "\n",
        "# Use fcluster to extract labels: labels\n",
        "labels = fcluster(mergings, 6, criterion='distance')\n",
        "\n",
        "# Create a DataFrame with labels and varieties as columns: df\n",
        "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
        "\n",
        "# Create crosstab: ct\n",
        "ct = pd.crosstab(df['labels'], df['varieties'])\n",
        "\n",
        "# Display ct\n",
        "print(ct)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jrXspO8w70sl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import TSNE\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Create a TSNE instance: model\n",
        "model = TSNE(learning_rate=200)\n",
        "\n",
        "# Apply fit_transform to samples: tsne_features\n",
        "tsne_features = model.fit_transform(samples)\n",
        "\n",
        "# Select the 0th feature: xs\n",
        "xs = tsne_features[:,0]\n",
        "\n",
        "# Select the 1st feature: ys\n",
        "ys = tsne_features[:,1]\n",
        "\n",
        "# Scatter plot, coloring by variety_numbers\n",
        "plt.scatter(xs, ys, c=variety_numbers)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "enAR2BWi72Wx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import TSNE\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Create a TSNE instance: model\n",
        "model = TSNE(learning_rate=50)\n",
        "\n",
        "# Apply fit_transform to normalized_movements: tsne_features\n",
        "tsne_features = model.fit_transform(normalized_movements)\n",
        "\n",
        "# Select the 0th feature: xs\n",
        "xs = tsne_features[:,0]\n",
        "\n",
        "# Select the 1th feature: ys\n",
        "ys = tsne_features[:,1]\n",
        "\n",
        "# Scatter plot\n",
        "plt.scatter(xs, ys, alpha=0.5)\n",
        "\n",
        "# Annotate the points\n",
        "for x, y, company in zip(xs, ys, companies):\n",
        "    plt.annotate(company, (x, y), fontsize=5, alpha=0.75)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zMLdSWZS8Ak5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#3"
      ]
    },
    {
      "metadata": {
        "id": "iggnXOL08B_O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Perform the necessary imports\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Assign the 0th column of grains: width\n",
        "width = grains[:, 0]\n",
        "\n",
        "# Assign the 1st column of grains: length\n",
        "length = grains[:, 1]\n",
        "\n",
        "# Scatter plot width vs length\n",
        "plt.scatter(width, length)\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "\n",
        "# Calculate the Pearson correlation\n",
        "correlation, pvalue = pearsonr(width, length)\n",
        "\n",
        "# Display the correlation\n",
        "print(correlation)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lOs0cv0E8GeO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Import PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create PCA instance: model\n",
        "model = PCA()\n",
        "\n",
        "# Apply the fit_transform method of model to grains: pca_features\n",
        "pca_features = model.fit_transform(grains)\n",
        "\n",
        "# Assign 0th column of pca_features: xs\n",
        "xs = pca_features[:,0]\n",
        "\n",
        "# Assign 1st column of pca_features: ys\n",
        "ys = pca_features[:,1]\n",
        "\n",
        "# Scatter plot xs vs ys\n",
        "plt.scatter(xs, ys)\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "\n",
        "# Calculate the Pearson correlation of xs and ys\n",
        "correlation, pvalue = pearsonr(xs, ys)\n",
        "\n",
        "# Display the correlation\n",
        "print(correlation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1dojTXqf8JLq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Make a scatter plot of the untransformed points\n",
        "plt.scatter(grains[:,0], grains[:,1])\n",
        "\n",
        "# Create a PCA instance: model\n",
        "model = PCA()\n",
        "\n",
        "# Fit model to points\n",
        "model.fit(grains)\n",
        "\n",
        "# Get the mean of the grain samples: mean\n",
        "mean = model.mean_\n",
        "\n",
        "# Get the first principal component: first_pc\n",
        "first_pc = model.components_[0,:]\n",
        "\n",
        "# Plot first_pc as an arrow, starting at mean\n",
        "plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)\n",
        "\n",
        "# Keep axes on same scale\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8pZEnOv_8J-R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Perform the necessary imports\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create scaler: scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Create a PCA instance: pca\n",
        "pca = PCA()\n",
        "\n",
        "# Create pipeline: pipeline\n",
        "pipeline = make_pipeline(scaler, pca)\n",
        "\n",
        "# Fit the pipeline to 'samples'\n",
        "pipeline.fit(samples)\n",
        "\n",
        "# Plot the explained variances\n",
        "features = range(pca.n_components_)\n",
        "plt.bar(features, pca.explained_variance_)\n",
        "plt.xlabel('PCA feature')\n",
        "plt.ylabel('variance')\n",
        "plt.xticks(features)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bByhkVqWG3L9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'Intrinsic dimension of the fish data\n",
        "In the previous exercise, you plotted the variance of the PCA features of the fish measurements. \n",
        "Looking again at your plot, what do you think would be a reasonable choice for the \"intrinsic dimension\" \n",
        "of the the fish measurements? Recall that the intrinsic dimension is the number of PCA features with significant variance.'\n",
        "\n",
        "1\n",
        "[2]\n",
        "5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kyW9HP858MNE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create a PCA model with 2 components: pca\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# Fit the PCA instance to the scaled samples\n",
        "pca.fit(scaled_samples)\n",
        "\n",
        "# Transform the scaled samples: pca_features\n",
        "pca_features = pca.transform(scaled_samples)\n",
        "\n",
        "# Print the shape of pca_features\n",
        "print(pca_features.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GO5xWOJ18Nyy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create a TfidfVectorizer: tfidf\n",
        "tfidf = TfidfVectorizer() \n",
        "\n",
        "# Apply fit_transform to document: csr_mat\n",
        "csr_mat = tfidf.fit_transform(documents)\n",
        "\n",
        "# Print result of toarray() method\n",
        "print(csr_mat.toarray())\n",
        "\n",
        "# Get the words: words\n",
        "words = tfidf.get_feature_names()\n",
        "\n",
        "# Print words\n",
        "print(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MAR-snkR8R5t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Perform the necessary imports\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Create a TruncatedSVD instance: svd\n",
        "svd = TruncatedSVD(n_components=50)\n",
        "\n",
        "# Create a KMeans instance: kmeans\n",
        "kmeans = KMeans(n_clusters=6)\n",
        "\n",
        "# Create a pipeline: pipeline\n",
        "pipeline = make_pipeline(svd, kmeans)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vSn-I8oL8S-T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Fit the pipeline to articles\n",
        "pipeline.fit(articles)\n",
        "\n",
        "# Calculate the cluster labels: labels\n",
        "labels = pipeline.predict(articles)\n",
        "\n",
        "# Create a DataFrame aligning labels and titles: df\n",
        "df = pd.DataFrame({'label': labels, 'article': titles})\n",
        "\n",
        "# Display df sorted by cluster label\n",
        "print(df.sort_values('label'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SId6Qw-xHbIz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'Non-negative data\n",
        "Which of the following 2-dimensional arrays are examples of non-negative data?\n",
        "\n",
        "A tf-idf word-frequency array.\n",
        "An array daily stock market price movements (up and down), where each row represents a company.\n",
        "An array where rows are customers, columns are products and entries are 0 or 1, indicating whether a customer has purchased a product.'\n",
        "\n",
        "1 only\n",
        "2 only 3\n",
        "[1 only 3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MknUXg_s8VIW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#4"
      ]
    },
    {
      "metadata": {
        "id": "zHjb-WRH8WIA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import NMF\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "# Create an NMF instance: model\n",
        "model = NMF(n_components=6)\n",
        "\n",
        "# Fit the model to articles\n",
        "model.fit(articles)\n",
        "\n",
        "# Transform the articles: nmf_features\n",
        "nmf_features = model.transform(articles)\n",
        "\n",
        "# Print the NMF features\n",
        "print(nmf_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ISiPWb5o9xPi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Create a pandas DataFrame: df\n",
        "df = pd.DataFrame(nmf_features, index=titles)\n",
        "\n",
        "# Print the row for 'Anne Hathaway'\n",
        "print(df.loc['Anne Hathaway'])\n",
        "\n",
        "# Print the row for 'Denzel Washington'\n",
        "print(df.loc['Denzel Washington'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uNYhoQBCH-Rb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'NMF reconstructs samples\n",
        "In this exercise, you'll \n",
        "check your understanding of how NMF reconstructs \n",
        "samples from its components using the NMF feature values. On the right are the components of an NMF model. If the NMF feature values of a sample are [2, 1], then which of the following is most likely to represent the original sample? \n",
        "A pen and paper will help here! You have to apply the same technique Ben used in the video to reconstruct the sample [0.1203 0.1764 0.3195 0.141].'\n",
        "\n",
        "[ [2.2, 1.0, 2.0] ]\n",
        "[0.5, 1.6, 3.1]\n",
        "[-4.0, 1.0, -2.0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IttJlDYI9y6T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame: components_df\n",
        "components_df = pd.DataFrame(model.components_, columns=words)\n",
        "\n",
        "# Print the shape of the DataFrame\n",
        "print(components_df.shape)\n",
        "\n",
        "# Select row 3: component\n",
        "component = components_df.iloc[3]\n",
        "\n",
        "# Print result of nlargest\n",
        "print(component.nlargest())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1W67JIUL90rR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import pyplot\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Select the 0th row: digit\n",
        "digit = samples[0, :]\n",
        "\n",
        "# Print digit\n",
        "print(digit)\n",
        "\n",
        "# Reshape digit to a 13x8 array: bitmap\n",
        "bitmap = digit.reshape((13, 8))\n",
        "\n",
        "# Print bitmap\n",
        "print(bitmap)\n",
        "\n",
        "# Use plt.imshow to display bitmap\n",
        "plt.imshow(bitmap, cmap='gray', interpolation='nearest')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xZIW4mIu95ql",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import NMF\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "# Create an NMF model: model\n",
        "model = NMF(n_components=7)\n",
        "\n",
        "# Apply fit_transform to samples: features\n",
        "features = model.fit_transform(samples)\n",
        "\n",
        "# Call show_as_image on each component\n",
        "for component in model.components_:\n",
        "    show_as_image(component)\n",
        "\n",
        "# Assign the 0th row of features: digit_features\n",
        "digit_features = features[0]\n",
        "\n",
        "# Print digit_features\n",
        "print(digit_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qcDNf4C097_W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create a PCA instance: model\n",
        "model = PCA(n_components=7)\n",
        "\n",
        "# Apply fit_transform to samples: features\n",
        "features = model.fit_transform(samples)\n",
        "\n",
        "# Call show_as_image on each component\n",
        "for component in model.components_:\n",
        "    show_as_image(component)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jZZrLj9s992U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Perform the necessary imports\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# Normalize the NMF features: norm_features\n",
        "norm_features = normalize(nmf_features)\n",
        "\n",
        "# Create a DataFrame: df\n",
        "df = pd.DataFrame(norm_features, index=titles)\n",
        "\n",
        "# Select the row corresponding to 'Cristiano Ronaldo': article\n",
        "article = df.loc['Cristiano Ronaldo']\n",
        "\n",
        "# Compute the dot products: similarities\n",
        "similarities = df.dot(article)\n",
        "\n",
        "# Display those with the largest cosine similarity\n",
        "print(similarities.nlargest())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5JRknL2E-EgP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Perform the necessary imports\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.preprocessing import Normalizer, MaxAbsScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Create a MaxAbsScaler: scaler\n",
        "scaler = MaxAbsScaler()\n",
        "\n",
        "# Create an NMF model: nmf\n",
        "nmf = NMF(n_components=20)\n",
        "\n",
        "# Create a Normalizer: normalizer\n",
        "normalizer = Normalizer()\n",
        "\n",
        "# Create a pipeline: pipeline\n",
        "pipeline = make_pipeline(scaler, nmf, normalizer)\n",
        "\n",
        "# Apply fit_transform to artists: norm_features\n",
        "norm_features = pipeline.fit_transform(artists)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ITyVIYGt-GzO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame: df\n",
        "df = pd.DataFrame(norm_features, index=artist_names)\n",
        "\n",
        "# Select row of 'Bruce Springsteen': artist\n",
        "artist = df.loc['Bruce Springsteen']\n",
        "\n",
        "# Compute cosine similarities: similarities\n",
        "similarities = df.dot(artist)\n",
        "\n",
        "# Display those with highest cosine similarity\n",
        "print(similarities.nlargest())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}